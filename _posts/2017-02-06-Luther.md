---
layout: post
date: 2017-02-06
title: Project Luther
---
## Web scraping and Regression

This project focused on a key tool in machine learning, also one of the most approachable. I've always like the math behind regression, it always had a very simple and intuitive nature to it. But going beyond that it becomes more difficult to talk about. Regression methods can predict continuous variables and there are many algorithms out there that preform regression. Linear regression is the simplest and most approachable.
We weren't given a specific dataset to pull from and were encourage to use multiple datasets to get unique results. My goal was to scrape IMDB.com and create a database of indipendent movies as well as block busters to compare the too. 

To preform the scraping I used pythons webscraping library BeautifulSoup and to preform the aggregation and analytics using pandas.

I collected as much data as I could and scraping more than skill and a good eye for detail, takes patience. I got at the maximum 10,000 datapoints and for each movie 10 feautures. This large dataset suffered from data corruption for certain values and I rescraped the sites again, only half with more robust feature scraping. This turned out a lot better.

Looking at the distribution of the data:

![alt](/images/blogLuther/imdbhist.png)  
![alt](/images/blogLuther/year_gross.png)    

Back to my initial goal: comparing indipendent movies and box office hits, this is not something I was equiped to deal with. 
While I can model one dataset and another, comparing the models is not straight forward. And for indipendent movies, I could only scrape 300 good datapoints, and after cleaning only 200 points, that is ony 200 movies and 10 features.

Here is a snapshot of the page:
![alt](/images/blogLuther/page1.png) 

Out of my features I decided to select only a few and to engineer some features to get a better fit. I tried to make sure the correlations were low for the features and took the logarithm of the budget and adjusted gross.
![alt](/images/blogLuther/corr.png) 

I decided, as a narrative to stick to for the sake of a presentation, that I would focus on modeling indipendent movies alone and preform a survey of different models and feature selection. The modeling and feature selection went well, and I preformed four solid regression models.

My models and their mean squared errors and r-squared values:

| Linear Regression: |  
| --------------- | ------------------- |
| Mean squared error: 0.17117544264192824 |  
| R-squared: 0.60954506696640465 |  

| Random forest: |  
| --------------- | ------------------- |
| Mean squared error: 0.12776655679724114 |  
| R-squared: 0.70856168613767989  |  

| Grid search random Forest: |  
| --------------- | ------------------- |
| Mean squared error: 0.0567824742567 | 
| R-squared: 0.991918963246 |  

| Gradient Boosted: |  
| --------------- | ------------------- |
| mean squared error: 0.13517333034433474  |  
| R-squared: 0.55631673556731542  |  

These results were a bit "cowboy," but they proved interesting to implement and see the overall tuning that I could do to get a better fit for my data. The best model was the gradient boosted random forest. The model's predictions versus the target is shown below:  
![alt](/images/blogLuther/pred_test.png)   



The model predicts fairly well. But to look at it further it would be best to look at the residuals:

![alt](/images/blogLuther/residual.png)   



